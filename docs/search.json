{
  "articles": [
    {
      "path": "about.html",
      "title": "National Park Service Data Release Reports",
      "description": "Resources and Guides for generating NPS DRRs associated with data packages\n",
      "author": [
        {
          "name": "Robert Baker",
          "url": {}
        },
        {
          "name": "Joe DeVivo",
          "url": {}
        },
        {
          "name": "Judd Patterson",
          "url": {}
        }
      ],
      "date": "November 20, 2022",
      "contents": "\r\nThe NPS DRR Template is an R Template and accompanying documentation to facilitate generating Data Release Reports associated with NPS packages for publication to DataStore.\r\nTo report bugs or request enhancements, please use post an issue on github or email Rob Baker\r\n\r\n\r\n\r\n",
      "last_modified": "2024-04-29T11:07:42-06:00"
    },
    {
      "path": "HowToUseThisTemplate.html",
      "title": "How To Use This Template",
      "author": [],
      "date": "20 November, 2022",
      "contents": "\r\n\r\nContents\r\nOverview\r\nProject Set-up\r\nFolder Structure\r\n\r\nReproducible Reports\r\nStandard Code Chunks\r\nCitations\r\nAutomating Citations\r\nManual citations\r\n\r\n\r\nEditing the Text\r\nData Records\r\nData Quality\r\nUsage Notes\r\nMethods\r\nReferences\r\nFigures\r\nTables\r\n\r\nPublishing DRRs\r\nReport Numbers\r\nLiability Statements\r\n\r\n\r\nOverview\r\nData Release Reports (DRRs) are created by the National Park Service and\r\nprovide detailed descriptions of valuable research datasets, including\r\nthe methods used to collect the data and technical analyses supporting\r\nthe quality of the measurements. Data Release Reports focus on helping\r\nothers reuse data, rather than presenting results, testing hypotheses,\r\nor presenting new interpretations, methods or in-depth analyses.\r\nDRRs are intended to document the processing of fully-QAed data to their\r\nfinal (QCed) form in a reproducible and transparent manner. DRRs\r\ndocument the data collection methods and quality standards used to\r\nprepare and review data prior to release. DRRs present the quality of\r\nresultant data in the context of fitness for their intended use.\r\nEach DRR cites source and resultant datasets that are published\r\nconcurrently and cross-referenced. Associated datasets are made publicly\r\navailable with the exception of data that must be protected from release\r\nas per NPS and park-specific policies.\r\nData packages that are published concurrently with DRRs are intended to\r\nbe independently citable scientific works that can serve as the basis\r\nfor subsequent analysis and reporting by NPS or third parties.\r\nProject Set-up\r\nThe Template is not a stand-alone file but instead has multiple\r\nassociated and dependent files. New projects can be established using by\r\ndownloading the Zip file associated with the latest\r\nrelease\r\nof the DRR template repository on GitHub.\r\nFolder Structure\r\nGeneral directory contents are as follows (Figure 1):\r\n\r\n\r\n\r\nFigure 1: Standard project directory structure for Data Release Reports\r\n\r\n\r\n\r\ninput This is the only location (other than the template script\r\nitself) where you you should manually change or edit files. This is\r\nwhere data files you need to supply should be placed.\r\nfigures If you have figures that are not generated as part of\r\nrunning this template, place those files here.\r\n\r\noutput Do not edit these files. This is where you can find your\r\nfinal products. Edits to files in this folder will not be\r\nincorporated when you knit the DRR_to_docx.rmd file. Any edits you\r\nmake to files in this directory may be overwritten when you knit the\r\ntemplate.\r\ntemp Do not edit these files. These are files that are either used\r\nor generated during when you knit the DRR_to_docx.rmd template. They\r\nmay include graphics, text, or data.\r\nYou can safely ignore most of the other folders (including docs,\r\nmy-website, vignettes,.github) and files (including\r\n_config.yml, .gitignore, .Rhistory, index.html, and\r\nnational-park-service-DRR.csl).\r\nReproducible Reports\r\nThe following is for users who are using the DRR_to_docx.rmd template\r\nfile to generate a data release report using RMarkdown.\r\nStandard Code Chunks\r\nIn addition to the report outline and a description of content for each\r\nsection, the template includes four standard code chunks.\r\nYAML Header:\r\nThe YAML header helps format the DRR. You should not need to edit any of\r\nthe YAML header.\r\nR code chunks:\r\nuser_edited_parameters. A series of parameters that are used in\r\nthe creation of the DRR and may be re-used in metadata and\r\nassociated data package construction. You will need to edit these\r\nparameters for each DRR.\r\ntitle. The title of the Data Release Report.\r\nreportNumber. This is optional, and should only be included\r\nif publishing in the semi-official DRR series. Set to NULL if\r\nthere is no reportNumber.\r\nDRR_DSRefID. This is the DataStore reference ID for the\r\nreport.\r\nauthorNames. A list of the author’s names.\r\nauthorAffiliations. A list of the author’s affiliations. The\r\norder of author affiliations must match the order of the authors\r\nin the authorNames list. Note that the entirety of each\r\naffiliation is enclosed in a single set of quotations. Line\r\nbreaks are indicated with the  tag. Do not worry about\r\nindentation or word wrapping. If two authors have the same\r\naffiliation, list the affiliation twice.\r\nauthorORCID. A list of ORCID iDs for each author in the format\r\n“xxxx-xxxx-xxxx-xxxx”. If an author does not have an ORCID iD,\r\nspecify NA (no quotes). The order of ORCID iDs (and NAs) must\r\ncorrespond to the order of authors in the authorNames list.\r\nFuture iterations of the DRR Template will pull ORCID iDs from\r\nmetadata and eventually from Active Directory. See\r\nORCID for more information about ORCID\r\niDs or to register an ORCID iD.\r\nDRRabstract. The abstract for the DRR (which may be distinct\r\nfrom the data package abstract). Pay careful attention to\r\nnon-standard characters, line breaks, carriage returns, and\r\ncurly-quotes. You may find it useful to write the abstract in\r\nNotePad or some other text editor and NOT a word processor (such\r\nas Microsoft Word). Indicate line breaks with and a space\r\nbetween paragraphs - should you want them - using . The\r\nAbstract should succinctly describe the study, the assay(s)\r\nperformed, the resulting data, and their reuse potential, but\r\nshould not make any claims regarding new scientific findings. No\r\nreferences are allowed in this section. A good suggested length\r\nfor abstracts is less than 250 words.\r\ndataPackageRefID. DataStore reference ID for the data package\r\nassociated with this report. You must have at least one data\r\npackage. Eventually, we will automate importing much of this\r\ninformation from metadata and include the ability to describe\r\nmultiple data packages in a single DRR.\r\ndataPackageTitle. The title of the data package. Must match\r\nthe title on DataStore (and metadata).\r\ndataPackageDescription. A short title/subtitle or short\r\ndescription for the data package. Must match the data package\r\nmetadata.\r\ndataPackageDOI. Auto-generated, no need to edit or update.\r\nThis is the data package DOI. It is based on the DataStore\r\nreference number.\r\ndataPackage_fileNames. List the file names in your data\r\npackage. Do NOT include metadata files. For example, include\r\n“my_data.csv” but do NOT include “my_metadata.xml”.\r\ndataPackage_fileSizes. List the approximate size of each data\r\nfile. Make sure the order of the file sizes corresponds to the\r\norder of file names in dataPackage_fileNames.\r\ndataPackage_fileDescript. A short description of the\r\ncorresponding data file that helps distinguish it from other\r\ndata files. A good guideline is 10 words or less. This will be\r\nused in a table summary table so brevity is a priority. If you\r\nhave already created metadata for your data package in EML\r\nformat, this should be the same text as found in the\r\n“entityDescription” element for each data file.\r\n\r\nsetup. Most users will not need to edit this code chunk. There is\r\none code snippet for loading packages; the RRpackages section is a\r\nsuite of packages that are used to assist with reproducible\r\nreporting. You may not need these for your report, but we have\r\nincluded them as part of the base recommended packages. If you plan\r\nto perform you QC as part of the DRR construction process, you can\r\nadd a second code snipped to import necessary packages for your QC\r\nprocess here.\r\ntitle_do_not_edit. These parameters are auto-generated based on\r\neither the EML you supplied (when that becomes an option) or the\r\ninformation you’ve already supplied under “user-edited-parameters”.\r\nYou really should not need to edit these parameters.\r\nauthors_do_not_edit. There is no need to edit this chunk. This\r\nwrites the author names, ORCID iDs, and affiliations to the .docx\r\ndocument based on information supplied in user-edited-parameters.\r\nLoadData. Any datasets you need to load can go here. For most\r\npeople these datasets are used to generate summary statistics on\r\nproportions of data that were flagged as accepted (A) accepted,\r\nestimated (AE) and rejected (R) during the quality control process.\r\nFileTable. Do not edit. Generates a table of file names, sizes,\r\nand descriptions in the data package being described by the DRR.\r\ndataFlaggingTable. This sample code provides a summary table\r\ndefining the suggested data flagging codes. There is no need to edit\r\nthis table.\r\nListing. Appendix A, by default is the code listing. This will\r\ngenerate all code used in generating the report and data packages.\r\nIn most cases, code listing is not required. If all QA/QC processes\r\nand data manipulations were performed elsewhere, you should cite\r\nthat code (in the methods and references) and leave the “listing\r\ncode chunk with the default settings of eval=FALSE and echo=FALSE.\r\nIf you have developed custom scripts, you can add those to DataStore\r\nwith the reference”Script” and cite them in the DRR.\r\nsession-info is the information about the versions of R and\r\npackages used in generating the report. In most cases, you do not\r\nneed to report session info (leave the session-info code chunk\r\nparameters in their default state: eval=FALSE). Session and version\r\ninformation is only necessary if you have set the “Listing” code\r\nchunk in appendix A to eval=TRUE. In that case, change the “session\r\ninfo” code chunk parameters to eval=TRUE.\r\nCitations\r\nAutomating Citations\r\nTo automate citations, add the citation in bibtex format to the file\r\n“references.bib”. You can manually copy and paste the bibtex for each\r\nreference in, or you can search for it from within Rstudio. From within\r\nRstudio, make sure you are editing this document using the “Visual” view\r\n(as opposed to “Source”). From the “Insert” drop-down menu, select “@\r\nCitation…” (shortcut: Cntrl-Shift-F8). This will open a Graphical User\r\nInterface (GUI) tool where you can view all the citations in your\r\nreference.bib file as well as search multiple databases for references,\r\nautomatically insert the bibtex for the reference into your\r\nreferences.bib file (and customize the unique identifier if you’d like)\r\nand insert the in-text citation into the DRR template.\r\n\r\n\r\n\r\nFigure 2: Adding Citations - Source vs. Visual editing of the Template and how to access the citation manager.\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 3: Adding Citations - Using the citation manager.\r\n\r\n\r\n\r\nOnce a reference is in your references.bib file, using the Visual view\r\nof the template you can simply type the ‘@’ symbol and select which\r\nreference to insert in the text.\r\nIf you need to edit how the citation is displayed after inserting it\r\ninto the text, switch back to the “Source” view. Each bibtex citation\r\nshould start with a unique identifier; the example reference in the\r\nsupplied references.bib file has the unique identifier\r\n“@article{Scott1994,”. Using the “Source” view in Rstudio, insert the\r\nreference in your text, by combining the “at” symbol with the portion of\r\nthe unique identifier after the curly bracket: @Scott1994 .\r\nSyntax\r\nResult\r\n@Scott1994 concludes that …\r\nScott et al., 1994 concludes that …\r\n@Scott1994[p.33] concludes that …\r\nScott (1994, p.33) concludes that …\r\n… end of sentence [@Scott1994].\r\n… end of sentence (Scott et al., 1994).\r\n… end of sentence [see @Scott1994,p.33].\r\n… end of sentence (see Scott et al. 1994,p.33).\r\ndelineate multiple authors with colon: [@Scott1994; @aberdeen1958]\r\ndelineate multiple authors with colon: (Scott et al., 1994; Aberdeen, 1958)\r\nScott et al. conclude that …. [-@Scott1994]\r\nScott et al. conclude that . . . (1994)\r\nThe full citation, properly formatted, will be included in a “References” section at the end of the rendered MS Word document. . . though it is also worth visually\r\ninspecting the .docx for citation completeness and formatting.\r\nManual citations\r\nIf you would like to format your citations manually, please feel free to\r\ndo so. Make sure to look at the References section for how to\r\nproperly format each citation type.\r\nEditing the Text\r\nThe following text in the body of the DRR template will need to be\r\nedited to customize it to each data package.\r\nData Records\r\nThis is a required section and consists of two subheadings:\r\nData inputs - an optional subsection used to describe datasets\r\nthat the data package is based on if it is a re-analysis,\r\nreorganization, or re-integration of prevously existing data sets.\r\nSummary of datasts created - this is a required section used to\r\nexplain each data record associated with the work (for instance, a\r\ndata package), including the DOI indicating where this information\r\nis stored. It shoudl also provide an overview of the data files and\r\ntheir formats. Each external data record should be cited.\r\nSample text is included that uses r code to incorporate previously\r\nspecified parameters such as the data package title, file names, and\r\nDOI.\r\nA code for a sample table summarizing the contents of the data package\r\n(except the metadata) is provided.\r\nData Quality\r\nThis is a required section. and the text includes multiple suggested\r\ntext elements and code for an example table defining data flagging\r\ncodes. Near future development here will incorporate additional optional\r\ntables to summarize the data quality based on the flags in the data\r\nsets.\r\nUsage Notes\r\nThis is a required section that should contain brief instructions to\r\nassist other researchers with reuse of the data. This may include\r\ndiscussion of software packages (with appropriate citations) that are\r\nsuitable for analysing the assay data files, suggested downstream\r\nprocessing steps (e.g. normalization, etc.), or tips for integrating or\r\ncomparing the data records with other datasets. Authors are encouraged\r\nto provide code, programs or data-processing workflows if they may help\r\nothers understand or use the data.\r\nMethods\r\nThis is a required section that cites previous methods used but should\r\nalso be detailed enough in describing data production including the\r\nexperimental design, data acquisition assays, and any computational\r\nprocessing (e.g. normalization, QA, QC) such that others can understand\r\nthe methods without referring to associated publications.\r\nOptional sub-sections within the methods include:\r\nData Collection and Sampling\r\nAdditional Data Sources\r\nData Processing\r\nCode availability\r\nReferences\r\nThis required section includes full bibliographic references for each\r\npaper, chapter, book, data package, dataset, protocol, etc cited within\r\nthe DRR.\r\nThere are numerous examples of proper formatting for each of these.\r\nFuture versions of the DRR will enable automatic reference formatting\r\ngiven a correctly formatted bibtex file with the references (.bib).\r\nFigures\r\nFigures should be inserted using code chunks in all cases so that figure\r\nsettings can be set in the chunk header. The chunk header should at a\r\nminimum set the fig.align parameter to “center” and the specify the\r\nfigure caption (fig.cap parameter). Inserting figures this way will\r\nensure that the caption is properly formatted and it will apply copy the\r\ncaption to the figure’s “alt text” tag, making it 508-compliant.\r\nFor example:\r\n```{r fig2, echo=FALSE, out.width=\"70%\", fig.align=\"center\", fig.cap=\"Example general workflow to incude in the methods section.\"} \r\nknitr::include_graphics(\"ProcessingWorkflow.png\")\r\n```\r\nResults in:\r\n\r\n\r\n\r\nFigure 4: Example general workflow to incude in the methods section.\r\n\r\n\r\n\r\nTables\r\nTables should be created using the kable function. Specifying the\r\ncaption in the kable function call (as opposed to inline markdown text)\r\nwill ensure that the caption is appropriately formatted.\r\nFor example:\r\n```{r Table2, echo=FALSE}\r\nc1<-c(\"Protocol1\",\"Protocol2\",\"Protocol3\")\r\nc2<-c(\"Park Unit 1\",\"Park Unit 2\",\"Park Unit 3\")\r\nc3<-c(\"Site 1\",\"Site 2\",\"Site 3\")\r\nc4<-c(\"Date 1\",\"Date 2\",\"Date 3\")\r\nc5<-c(\"GEOXXXXX\",\"GEOXXXXX\",\"GEOXXXXX\")\r\nTable2<-data.frame(c1,c2,c3,c4,c5)\r\n\r\nkable(Table2, \r\n      col.names=c(\"Subjects\",\"Park Units\",\"Locations\",\"Sampling Dates\",\"Data\"),\r\n      caption=\"**Table 1.** Monitoring study example Data Records table.\") %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),full_width=F)\r\n```\r\nResults in:\r\n\r\n\r\nTable 1: Table 1. Monitoring study example Data Records table.\r\n\r\n\r\nSubjects\r\n\r\n\r\nPark Units\r\n\r\n\r\nLocations\r\n\r\n\r\nSampling Dates\r\n\r\n\r\nData\r\n\r\n\r\nProtocol1\r\n\r\n\r\nPark Unit 1\r\n\r\n\r\nSite 1\r\n\r\n\r\nDate 1\r\n\r\n\r\nGEOXXXXX\r\n\r\n\r\nProtocol2\r\n\r\n\r\nPark Unit 2\r\n\r\n\r\nSite 2\r\n\r\n\r\nDate 2\r\n\r\n\r\nGEOXXXXX\r\n\r\n\r\nProtocol3\r\n\r\n\r\nPark Unit 3\r\n\r\n\r\nSite 3\r\n\r\n\r\nDate 3\r\n\r\n\r\nGEOXXXXX\r\n\r\n\r\nPublishing DRRs\r\nReport Numbers\r\nBecause data release reports and associated data packages are\r\ncross-referential, report numbers are typically assigned early in data\r\nprocessing and quality evaluation.\r\nDataStore Reference Numbers. When developing a report and data\r\npackages, DataStore references should be created as early in the\r\nprocess as practicable. While the report and data packages are in\r\ndevelopment, these should not be activated.\r\nReport Numbers. If you are planning to publish a Data Release\r\nReport with an official DRR number, please contact the IMD Deputy\r\nChief with the DataStore reference number associated with the DRR.\r\nPersistent Identifiers. Digital object identifiers (DOIs) will\r\nbe assigned to all DRRs and concurrently published data packages.\r\nDOIs will resolve to a DataStore Reference; DOIs are reserved when a\r\ndraft reference is initiated in DataStore. They are not activated\r\nuntil the publication process, including relevant review, is\r\ncomplete.\r\nDRR DOIs have the format: https://doi.org/10.36967/xxxxxxx\r\nData package DOIs have the format: https://doi.org/10.57830/xxxxxxx\r\nWhere the “xxxxxx” is the 7-digit DataStore reference number.\r\nLiability Statements\r\nUnder no circumstances should reports and associated data packages or\r\nmetadata published in the DRR series contain disclaimers or text that\r\nsuggests that the work does not meet scientific integrity or information\r\nquality standards of the National Park Service. The following\r\ndisclaimers are suitable for use, depending on whether the data are\r\nprovisional or final (or approved or certified).\r\n\r\nFor approved & published data sets: “Unless otherwise stated, all\r\ndata, metadata and related materials are considered to satisfy the\r\nquality standards relative to the purpose for which the data were\r\ncollected. Although these data and associated metadata have been\r\nreviewed for accuracy and completeness and approved for release by the\r\nNational Park Service Inventory and Monitoring Division, no warranty\r\nexpressed or implied is made regarding the display or utility of the\r\ndata for other purposes, nor on all computer systems, nor shall the\r\nact of distribution constitute any such warranty.”\r\n\r\n\r\nFor provisional data: “The data you have secured from the National\r\nPark Service (NPS) database identified as [database name] have not\r\nreceived approval for release by the NPS Inventory and Monitoring\r\nDivision, and as such are provisional and subject to revision. The\r\ndata are released on the condition that neither the NPS nor the U.S.\r\nGovernment shall be held liable for any damages resulting from its\r\nauthorized or unauthorized use.”\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-04-29T11:07:46-06:00"
    },
    {
      "path": "HowToUseThisTemplate.html",
      "title": "National Park Service Data Release Reports",
      "description": "Resources and Guides for generating NPS DRRs associated with data packages\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nOverview\r\nProject Set-up\r\nFolder Structure\r\n\r\nReproducible Reports\r\nStandard Code Chunks\r\nCitations\r\nAutomating Citations\r\nManual citations\r\n\r\n\r\nEditing the Text\r\nData Records\r\nData Quality\r\nUsage Notes\r\nMethods\r\nReferences\r\nFigures\r\nTables\r\n\r\nPublishing DRRs\r\nReport Numbers\r\nLiability Statements\r\n\r\n\r\nOverview\r\nData Release Reports (DRRs) are created by the National Park Service and\r\nprovide detailed descriptions of valuable research datasets, including\r\nthe methods used to collect the data and technical analyses supporting\r\nthe quality of the measurements. Data Release Reports focus on helping\r\nothers reuse data, rather than presenting results, testing hypotheses,\r\nor presenting new interpretations, methods or in-depth analyses.\r\nDRRs are intended to document the processing of fully-QAed data to their\r\nfinal (QCed) form in a reproducible and transparent manner. DRRs\r\ndocument the data collection methods and quality standards used to\r\nprepare and review data prior to release. DRRs present the quality of\r\nresultant data in the context of fitness for their intended use.\r\nEach DRR cites source and resultant datasets that are published\r\nconcurrently and cross-referenced. Associated datasets are made publicly\r\navailable with the exception of data that must be protected from release\r\nas per NPS and park-specific policies.\r\nData packages that are published concurrently with DRRs are intended to\r\nbe independently citable scientific works that can serve as the basis\r\nfor subsequent analysis and reporting by NPS or third parties.\r\nProject Set-up\r\nThe Template is not a stand-alone file but instead has multiple\r\nassociated and dependent files. New projects can be established using by\r\ndownloading the Zip file associated with the latest\r\nrelease\r\nof the DRR template repository on GitHub.\r\nFolder Structure\r\nGeneral directory contents are as follows (Figure 1):\r\n\r\n\r\n\r\nFigure 1: Standard project directory structure for Data Release Reports\r\n\r\n\r\n\r\ninput This is the only location (other than the template script\r\nitself) where you you should manually change or edit files. This is\r\nwhere data files you need to supply should be placed.\r\nfigures If you have figures that are not generated as part of\r\nrunning this template, place those files here.\r\n\r\noutput Do not edit these files. This is where you can find your\r\nfinal products. Edits to files in this folder will not be\r\nincorporated when you knit the DRR_to_docx.rmd file. Any edits you\r\nmake to files in this directory may be overwritten when you knit the\r\ntemplate.\r\ntemp Do not edit these files. These are files that are either used\r\nor generated during when you knit the DRR_to_docx.rmd template. They\r\nmay include graphics, text, or data.\r\nYou can safely ignore most of the other folders (including docs,\r\nmy-website, vignettes,.github) and files (including\r\n_config.yml, .gitignore, .Rhistory, index.html, and\r\nnational-park-service-DRR.csl).\r\nReproducible Reports\r\nThe following is for users who are using the DRR_to_docx.rmd template\r\nfile to generate a data release report using RMarkdown.\r\nStandard Code Chunks\r\nIn addition to the report outline and a description of content for each\r\nsection, the template includes four standard code chunks.\r\nYAML Header:\r\nThe YAML header helps format the DRR. You should not need to edit any of\r\nthe YAML header.\r\nR code chunks:\r\nuser_edited_parameters. A series of parameters that are used in\r\nthe creation of the DRR and may be re-used in metadata and\r\nassociated data package construction. You will need to edit these\r\nparameters for each DRR.\r\ntitle. The title of the Data Release Report.\r\nreportNumber. This is optional, and should only be included\r\nif publishing in the semi-official DRR series. Set to NULL if\r\nthere is no reportNumber.\r\nDRR_DSRefID. This is the DataStore reference ID for the\r\nreport.\r\nauthorNames. A list of the author’s names.\r\nauthorAffiliations. A list of the author’s affiliations. The\r\norder of author affiliations must match the order of the authors\r\nin the authorNames list. Note that the entirety of each\r\naffiliation is enclosed in a single set of quotations. Line\r\nbreaks are indicated with the  tag. Do not worry about\r\nindentation or word wrapping. If two authors have the same\r\naffiliation, list the affiliation twice.\r\nauthorORCID. A list of ORCID iDs for each author in the format\r\n“xxxx-xxxx-xxxx-xxxx”. If an author does not have an ORCID iD,\r\nspecify NA (no quotes). The order of ORCID iDs (and NAs) must\r\ncorrespond to the order of authors in the authorNames list.\r\nFuture iterations of the DRR Template will pull ORCID iDs from\r\nmetadata and eventually from Active Directory. See\r\nORCID for more information about ORCID\r\niDs or to register an ORCID iD.\r\nDRRabstract. The abstract for the DRR (which may be distinct\r\nfrom the data package abstract). Pay careful attention to\r\nnon-standard characters, line breaks, carriage returns, and\r\ncurly-quotes. You may find it useful to write the abstract in\r\nNotePad or some other text editor and NOT a word processor (such\r\nas Microsoft Word). Indicate line breaks with and a space\r\nbetween paragraphs - should you want them - using . The\r\nAbstract should succinctly describe the study, the assay(s)\r\nperformed, the resulting data, and their reuse potential, but\r\nshould not make any claims regarding new scientific findings. No\r\nreferences are allowed in this section. A good suggested length\r\nfor abstracts is less than 250 words.\r\ndataPackageRefID. DataStore reference ID for the data package\r\nassociated with this report. You must have at least one data\r\npackage. Eventually, we will automate importing much of this\r\ninformation from metadata and include the ability to describe\r\nmultiple data packages in a single DRR.\r\ndataPackageTitle. The title of the data package. Must match\r\nthe title on DataStore (and metadata).\r\ndataPackageDescription. A short title/subtitle or short\r\ndescription for the data package. Must match the data package\r\nmetadata.\r\ndataPackageDOI. Auto-generated, no need to edit or update.\r\nThis is the data package DOI. It is based on the DataStore\r\nreference number.\r\ndataPackage_fileNames. List the file names in your data\r\npackage. Do NOT include metadata files. For example, include\r\n“my_data.csv” but do NOT include “my_metadata.xml”.\r\ndataPackage_fileSizes. List the approximate size of each data\r\nfile. Make sure the order of the file sizes corresponds to the\r\norder of file names in dataPackage_fileNames.\r\ndataPackage_fileDescript. A short description of the\r\ncorresponding data file that helps distinguish it from other\r\ndata files. A good guideline is 10 words or less. This will be\r\nused in a table summary table so brevity is a priority. If you\r\nhave already created metadata for your data package in EML\r\nformat, this should be the same text as found in the\r\n“entityDescription” element for each data file.\r\n\r\nsetup. Most users will not need to edit this code chunk. There is\r\none code snippet for loading packages; the RRpackages section is a\r\nsuite of packages that are used to assist with reproducible\r\nreporting. You may not need these for your report, but we have\r\nincluded them as part of the base recommended packages. If you plan\r\nto perform you QC as part of the DRR construction process, you can\r\nadd a second code snipped to import necessary packages for your QC\r\nprocess here.\r\ntitle_do_not_edit. These parameters are auto-generated based on\r\neither the EML you supplied (when that becomes an option) or the\r\ninformation you’ve already supplied under “user-edited-parameters”.\r\nYou really should not need to edit these parameters.\r\nauthors_do_not_edit. There is no need to edit this chunk. This\r\nwrites the author names, ORCID iDs, and affiliations to the .docx\r\ndocument based on information supplied in user-edited-parameters.\r\nLoadData. Any datasets you need to load can go here. For most\r\npeople these datasets are used to generate summary statistics on\r\nproportions of data that were flagged as accepted (A) accepted,\r\nestimated (AE) and rejected (R) during the quality control process.\r\nFileTable. Do not edit. Generates a table of file names, sizes,\r\nand descriptions in the data package being described by the DRR.\r\ndataFlaggingTable. This sample code provides a summary table\r\ndefining the suggested data flagging codes. There is no need to edit\r\nthis table.\r\nListing. Appendix A, by default is the code listing. This will\r\ngenerate all code used in generating the report and data packages.\r\nIn most cases, code listing is not required. If all QA/QC processes\r\nand data manipulations were performed elsewhere, you should cite\r\nthat code (in the methods and references) and leave the “listing\r\ncode chunk with the default settings of eval=FALSE and echo=FALSE.\r\nIf you have developed custom scripts, you can add those to DataStore\r\nwith the reference”Script” and cite them in the DRR.\r\nsession-info is the information about the versions of R and\r\npackages used in generating the report. In most cases, you do not\r\nneed to report session info (leave the session-info code chunk\r\nparameters in their default state: eval=FALSE). Session and version\r\ninformation is only necessary if you have set the “Listing” code\r\nchunk in appendix A to eval=TRUE. In that case, change the “session\r\ninfo” code chunk parameters to eval=TRUE.\r\nCitations\r\nAutomating Citations\r\nTo automate citations, add the citation in bibtex format to the file\r\n“references.bib”. You can manually copy and paste the bibtex for each\r\nreference in, or you can search for it from within Rstudio. From within\r\nRstudio, make sure you are editing this document using the “Visual” view\r\n(as opposed to “Source”). From the “Insert” drop-down menu, select “@\r\nCitation…” (shortcut: Cntrl-Shift-F8). This will open a Graphical User\r\nInterface (GUI) tool where you can view all the citations in your\r\nreference.bib file as well as search multiple databases for references,\r\nautomatically insert the bibtex for the reference into your\r\nreferences.bib file (and customize the unique identifier if you’d like)\r\nand insert the in-text citation into the DRR template.\r\n\r\n\r\n\r\nFigure 2: Adding Citations - Source vs. Visual editing of the Template and how to access the citation manager.\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 3: Adding Citations - Using the citation manager.\r\n\r\n\r\n\r\nOnce a reference is in your references.bib file, using the Visual view\r\nof the template you can simply type the ‘@’ symbol and select which\r\nreference to insert in the text.\r\nIf you need to edit how the citation is displayed after inserting it\r\ninto the text, switch back to the “Source” view. Each bibtex citation\r\nshould start with a unique identifier; the example reference in the\r\nsupplied references.bib file has the unique identifier\r\n“@article{Scott1994,”. Using the “Source” view in Rstudio, insert the\r\nreference in your text, by combining the “at” symbol with the portion of\r\nthe unique identifier after the curly bracket: @Scott1994 .\r\nSyntax\r\nResult\r\n@Scott1994 concludes that …\r\nScott et al., 1994 concludes that …\r\n@Scott1994[p.33] concludes that …\r\nScott (1994, p.33) concludes that …\r\n… end of sentence [@Scott1994].\r\n… end of sentence (Scott et al., 1994).\r\n… end of sentence [see @Scott1994,p.33].\r\n… end of sentence (see Scott et al. 1994,p.33).\r\ndelineate multiple authors with colon: [@Scott1994; @aberdeen1958]\r\ndelineate multiple authors with colon: (Scott et al., 1994; Aberdeen, 1958)\r\nScott et al. conclude that …. [-@Scott1994]\r\nScott et al. conclude that . . . (1994)\r\nThe full citation, properly formatted, will be included in a “References” section at the end of the rendered MS Word document. . . though it is also worth visually\r\ninspecting the .docx for citation completeness and formatting.\r\nManual citations\r\nIf you would like to format your citations manually, please feel free to\r\ndo so. Make sure to look at the References section for how to\r\nproperly format each citation type.\r\nEditing the Text\r\nThe following text in the body of the DRR template will need to be\r\nedited to customize it to each data package.\r\nData Records\r\nThis is a required section and consists of two subheadings:\r\nData inputs - an optional subsection used to describe datasets\r\nthat the data package is based on if it is a re-analysis,\r\nreorganization, or re-integration of prevously existing data sets.\r\nSummary of datasts created - this is a required section used to\r\nexplain each data record associated with the work (for instance, a\r\ndata package), including the DOI indicating where this information\r\nis stored. It shoudl also provide an overview of the data files and\r\ntheir formats. Each external data record should be cited.\r\nSample text is included that uses r code to incorporate previously\r\nspecified parameters such as the data package title, file names, and\r\nDOI.\r\nA code for a sample table summarizing the contents of the data package\r\n(except the metadata) is provided.\r\nData Quality\r\nThis is a required section. and the text includes multiple suggested\r\ntext elements and code for an example table defining data flagging\r\ncodes. Near future development here will incorporate additional optional\r\ntables to summarize the data quality based on the flags in the data\r\nsets.\r\nUsage Notes\r\nThis is a required section that should contain brief instructions to\r\nassist other researchers with reuse of the data. This may include\r\ndiscussion of software packages (with appropriate citations) that are\r\nsuitable for analysing the assay data files, suggested downstream\r\nprocessing steps (e.g. normalization, etc.), or tips for integrating or\r\ncomparing the data records with other datasets. Authors are encouraged\r\nto provide code, programs or data-processing workflows if they may help\r\nothers understand or use the data.\r\nMethods\r\nThis is a required section that cites previous methods used but should\r\nalso be detailed enough in describing data production including the\r\nexperimental design, data acquisition assays, and any computational\r\nprocessing (e.g. normalization, QA, QC) such that others can understand\r\nthe methods without referring to associated publications.\r\nOptional sub-sections within the methods include:\r\nData Collection and Sampling\r\nAdditional Data Sources\r\nData Processing\r\nCode availability\r\nReferences\r\nThis required section includes full bibliographic references for each\r\npaper, chapter, book, data package, dataset, protocol, etc cited within\r\nthe DRR.\r\nThere are numerous examples of proper formatting for each of these.\r\nFuture versions of the DRR will enable automatic reference formatting\r\ngiven a correctly formatted bibtex file with the references (.bib).\r\nFigures\r\nFigures should be inserted using code chunks in all cases so that figure\r\nsettings can be set in the chunk header. The chunk header should at a\r\nminimum set the fig.align parameter to “center” and the specify the\r\nfigure caption (fig.cap parameter). Inserting figures this way will\r\nensure that the caption is properly formatted and it will apply copy the\r\ncaption to the figure’s “alt text” tag, making it 508-compliant.\r\nFor example:\r\n```{r fig2, echo=FALSE, out.width=\"70%\", fig.align=\"center\", fig.cap=\"Example general workflow to incude in the methods section.\"} \r\nknitr::include_graphics(\"ProcessingWorkflow.png\")\r\n```\r\nResults in:\r\n\r\n\r\n\r\nFigure 4: Example general workflow to incude in the methods section.\r\n\r\n\r\n\r\nTables\r\nTables should be created using the kable function. Specifying the\r\ncaption in the kable function call (as opposed to inline markdown text)\r\nwill ensure that the caption is appropriately formatted.\r\nFor example:\r\n```{r Table2, echo=FALSE}\r\nc1<-c(\"Protocol1\",\"Protocol2\",\"Protocol3\")\r\nc2<-c(\"Park Unit 1\",\"Park Unit 2\",\"Park Unit 3\")\r\nc3<-c(\"Site 1\",\"Site 2\",\"Site 3\")\r\nc4<-c(\"Date 1\",\"Date 2\",\"Date 3\")\r\nc5<-c(\"GEOXXXXX\",\"GEOXXXXX\",\"GEOXXXXX\")\r\nTable2<-data.frame(c1,c2,c3,c4,c5)\r\n\r\nkable(Table2, \r\n      col.names=c(\"Subjects\",\"Park Units\",\"Locations\",\"Sampling Dates\",\"Data\"),\r\n      caption=\"**Table 1.** Monitoring study example Data Records table.\") %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),full_width=F)\r\n```\r\nResults in:\r\n\r\n\r\nTable 1: Table 1. Monitoring study example Data Records table.\r\n\r\n\r\nSubjects\r\n\r\n\r\nPark Units\r\n\r\n\r\nLocations\r\n\r\n\r\nSampling Dates\r\n\r\n\r\nData\r\n\r\n\r\nProtocol1\r\n\r\n\r\nPark Unit 1\r\n\r\n\r\nSite 1\r\n\r\n\r\nDate 1\r\n\r\n\r\nGEOXXXXX\r\n\r\n\r\nProtocol2\r\n\r\n\r\nPark Unit 2\r\n\r\n\r\nSite 2\r\n\r\n\r\nDate 2\r\n\r\n\r\nGEOXXXXX\r\n\r\n\r\nProtocol3\r\n\r\n\r\nPark Unit 3\r\n\r\n\r\nSite 3\r\n\r\n\r\nDate 3\r\n\r\n\r\nGEOXXXXX\r\n\r\n\r\nPublishing DRRs\r\nReport Numbers\r\nBecause data release reports and associated data packages are\r\ncross-referential, report numbers are typically assigned early in data\r\nprocessing and quality evaluation.\r\nDataStore Reference Numbers. When developing a report and data\r\npackages, DataStore references should be created as early in the\r\nprocess as practicable. While the report and data packages are in\r\ndevelopment, these should not be activated.\r\nReport Numbers. If you are planning to publish a Data Release\r\nReport with an official DRR number, please contact the IMD Deputy\r\nChief with the DataStore reference number associated with the DRR.\r\nPersistent Identifiers. Digital object identifiers (DOIs) will\r\nbe assigned to all DRRs and concurrently published data packages.\r\nDOIs will resolve to a DataStore Reference; DOIs are reserved when a\r\ndraft reference is initiated in DataStore. They are not activated\r\nuntil the publication process, including relevant review, is\r\ncomplete.\r\nDRR DOIs have the format: https://doi.org/10.36967/xxxxxxx\r\nData package DOIs have the format: https://doi.org/10.57830/xxxxxxx\r\nWhere the “xxxxxx” is the 7-digit DataStore reference number.\r\nLiability Statements\r\nUnder no circumstances should reports and associated data packages or\r\nmetadata published in the DRR series contain disclaimers or text that\r\nsuggests that the work does not meet scientific integrity or information\r\nquality standards of the National Park Service. The following\r\ndisclaimers are suitable for use, depending on whether the data are\r\nprovisional or final (or approved or certified).\r\n\r\nFor approved & published data sets: “Unless otherwise stated, all\r\ndata, metadata and related materials are considered to satisfy the\r\nquality standards relative to the purpose for which the data were\r\ncollected. Although these data and associated metadata have been\r\nreviewed for accuracy and completeness and approved for release by the\r\nNational Park Service Inventory and Monitoring Division, no warranty\r\nexpressed or implied is made regarding the display or utility of the\r\ndata for other purposes, nor on all computer systems, nor shall the\r\nact of distribution constitute any such warranty.”\r\n\r\n\r\nFor provisional data: “The data you have secured from the National\r\nPark Service (NPS) database identified as [database name] have not\r\nreceived approval for release by the NPS Inventory and Monitoring\r\nDivision, and as such are provisional and subject to revision. The\r\ndata are released on the condition that neither the NPS nor the U.S.\r\nGovernment shall be held liable for any damages resulting from its\r\nauthorized or unauthorized use.”\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-04-29T11:07:46-06:00"
    },
    {
      "path": "index.html",
      "title": "National Park Service Data Release Reports",
      "description": "Resources and Guides for generating NPS DRRs associated with data packages\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nThis repo has been deprecated\r\n\r\n\r\n\r\n\r\nThis repo has been deprecated\r\nThe scripts and documentation in this repo are no longer being maintained and are very likely out of date.\r\nThe contents of this repo including the EML creation script and all documentation can now be found in the R package QCkit, which is part of the NPSdataverse.\r\n\r\n\r\n\r\n",
      "last_modified": "2024-04-29T11:07:48-06:00"
    },
    {
      "path": "PurposeAndScope.html",
      "title": "Purpose and Scope of Data Release Reports",
      "author": [],
      "date": "20 November, 2022",
      "contents": "\r\n\r\nContents\r\nBackground\r\nDefinitions\r\nPolicy Requirements\r\nNPS Requirements\r\nNPS Guidelines\r\nI&M Requirements\r\n\r\nImplications\r\nScope\r\nGeneral Studies\r\nVital Signs Monitoring\r\nInventory Studies\r\n\r\n\r\nBackground\r\nRecognizing the broad move toward open science, we’ve seen the following changes\r\nin expectations (outside NPS) since the inception of the I&M program.\r\nBroad adoption of open-data and open-by-default practices.\r\nA move in the scientific disciplines toward considering and publishing data\r\nsets as independently-citable scientific works.\r\nRoutine assignment of digital object identifiers (DOIs) to datasets to facilitate location, reuse, and\r\ncitation of specific data\r\nIncreased transparency and reproducibility in the processing and analysis of\r\ndata.\r\nEstablishment of peer reviewed “data journals” dedicated to publishing data sets\r\nand associated documentation designed to facilitate their reuse.\r\nExpectation that science-based decisions are based on peer-reviewed,\r\nreproducible, and open science by default.\r\nData release reports are designed to parallel external peer-reviewed scientific\r\njournals dedicated to facilitate reuse of reproducible scientific data, in\r\nrecognition that the primary reason IMD data are collected is to support\r\nscience-based decisions.\r\nNote that publication in a data release report series (not mandated) is distinct\r\nfrom requirements to document data collection, processing, and quality\r\nevaluation (mandated; see below). The establishment of a data release report\r\nseries is intended to facilitate and encourage this type of reporting in a\r\nstandard format, and in a manner commensurate with current scientific norms.\r\nA template for creating data release reports to be published in the DRR series. We have also\r\ndeveloped procedures for authoring DRR reports in Microsoft Word and porting\r\nthem to the appropriate format.\r\nDefinitions\r\nReproducibility. The degree to which scientific information, modeling, and\r\nmethods of analysis could be evaluated by an independent third party to arrive\r\nat the same, or substantially similar, conclusion as the original study or\r\ninformation, and that the scientific assessment can be repeated to obtain\r\nsimilar results (Plesser 2017). A study is reproducible if you can take the original data and the\r\ncomputer code used to analyze the data and reproduce all of the numerical findings\r\nfrom the study. This may initially sound like a trivial task but experience has shown that\r\nit’s not always easy to achieve this seemingly minimal standard (ASA 2017, Plesser 2017).\r\nTransparency. Full disclosure of the methods used to obtain, process,\r\nanalyze, and review scientific data and other information products, the\r\navailability of the data that went into and came out of the analysis, and the\r\ncomputer code used to conduct the analysis. Documenting this information is\r\ncrucial to ensure reproducibility and requires, at minimum, the sharing of\r\nanalytical data sets, relevant metadata, analytical code, and related software.\r\nFitness for Use. The utility of scientific information (in this case a\r\ndataset) for its intended users and its intended purposes. Agencies must review\r\nand communicate the fitness of a dataset for its intended purpose, and should\r\nprovide the public sufficient documentation about each dataset to allow data\r\nusers to determine the fitness of the data for the purpose for which third\r\nparties may consider using it.\r\nDecisions. The type of decisions that must be based on publicly-available,\r\nreproducible, and peer-reviewed science has not been defined. At a minimum it\r\nincludes any influential decisions, but it may also include any decisions\r\nsubject to public review and comment.\r\nDescriptive Reporting. The policies listed above are consistent in the\r\nrequirement to provide documentation that describes the methods used to collect,\r\nprocess, and evaluate science products, including data. Note that this is\r\ndistinct from (and in practice may significantly differ from) prescriptive\r\ndocuments such as protocols, procedures, and study plans. Descriptive reporting\r\nshould cite or describe relevant science planning documents, methods used,\r\ndeviations, and mitigations. In total, descriptive reporting provides a clear\r\n“line of sight” on precisely how data were collected, processed, and evaluated.\r\nAlthough deviations may warrant revisions to prescriptive documents, changes in\r\nprescriptive documents after the fact do not meet reproducibility and\r\ntransparency requirements.\r\nPolicy Requirements\r\nNPS Requirements\r\nDO11B-a, DO 11B-b, OMB M-05-03 (Peer review and information quality):\r\nScientific information must be appropriately reviewed prior to use in\r\ndecision-making, regulatory processes, or dissemination to the public,\r\nregardless of media.\r\nAs per OMB M-05-03 “scientific information” includes factual inputs, data,\r\nmodels, analyses, technical information, or scientific assessments related\r\nto such disciplines as the behavioral and social sciences, public health and\r\nmedical sciences, life and earth sciences, engineering, or physical\r\nsciences.\r\nMethods for producing information will be made transparent, to the maximum\r\nextent practicable, through accurate documentation, use of appropriate\r\nreview, and verification of information quality.\r\nOMB M-19-15 (Updates to Implementing the Information Quality Act):\r\nFederal agencies must collect, use, and disseminate information that is fit\r\nfor its intended purpose.\r\nAgencies must conduct pre-dissemination review of quality [of scientific\r\ninformation] based on the likely use of that information. Quality\r\nencompasses utility, integrity, and objectivity, defined as follows: a)\r\nUtility – utility for its intended users and its intended purposes, b)\r\nIntegrity – refers to security, and c) Objectivity – accurate, reliable, and\r\nunbiased as a matter of presentation and substance.\r\nAgencies should provide the public with sufficient documentation about each\r\ndataset released to allow data users to determine the fitness of the data\r\nfor the purpose for which third parties may consider using it. Potential\r\nusers must be provided with sufficient information to understand… the\r\ndata’s strengths, weaknesses, analytical limitations, security requirements,\r\nand processing options.\r\nReproducibility requirements for Influential Information. Note that because\r\nthis may not be determined at the time of collection, processing, or\r\ndissemination this should be the default for NPS scientific activities:\r\nAnalyses must be disseminated with sufficient descriptions of data and\r\nmethods to allow them to be reproduced by qualified third parties who\r\nmay want to test the sensitivity of agency analyses. This is a higher\r\nstandard than simply documenting the characteristics of the underlying\r\ndata, which is required for all information.\r\nComputer code used to process data should be made available to the\r\npublic for further analysis. In the context of results generated, for\r\nexample, a statistical model or machine augmented learning and decision\r\nsupport, reproducibility requires, at a minimum transparency about the\r\nspecific methods, design parameters, equations or algorithms,\r\nparameters, and assumptions used.\r\n\r\nReports, data, and computer code used, developed, or cited in the analysis\r\nand reporting of findings must be made publicly available except where\r\nprohibited by law.\r\nNPS Guidelines\r\nNPS guidelines on Use of Scientific Information\r\nMultiple policy and guidance documents require the use of best available science in decision-making. Additional requirements include:\r\nSO 3369 (Promoting Open Science):\r\nDefines “best available science” as publicly-available, reproducible, and peer reviewed. Requires that any decisions or scientific conclusions must prioritize the use of publicly-available, reproducible, and peer-reviewed science. Decisions or conclusions not based on such must include an explanation of why the alternative is the best available information. Effective as of 28 September 2018 with no transition period.\r\nDO 11B (Ensuring Objectivity, Utility, and Integrity of Information Used and Disseminated by the National Park Service):\r\nThe NPS will ensure that information it releases to the public or utilizes in management decisions will be developed from reliable data sources that provide the highest quality of information at each stage of information development.\r\nI&M Requirements\r\nNPS-75 (Inventory and Monitoring Guidelines):\r\nAn annual summary report documenting the condition of park resources should\r\nbe developed as part of the annual revision of the parks Resource Management\r\nPlan.\r\nAn annual report provides a mechanism for reviewing and making\r\nrecommendations for revisions in the [Protocol/SOPs].\r\n[Inventory] data obtained should be archived in park records and, when\r\nappropriate, a report should be written summarizing findings.\r\nReporting requirements as per IMD directive\r\nIMD Reporting and Analysis Guidance\r\nAnnual Analyses Required of all Monitoring Protocols: Conduct an annual data review\r\nto address whether there is any unexpected variability or outliers, and\r\nwhether any protocol changes or additional studies may be needed. Part of\r\nthe review must assess the data against standards defined in the protocol\r\nnarrative, data quality standards document or quality assurance plan, and\r\ndocument whether those standards were met. When data are not available for\r\nreview during the year they are collected (for example, when data have been\r\nsubmitted to a lab for analysis), review must be conducted the year the data\r\nare available. For example, if water quality and quantity data are typically\r\nreviewed in October and lab results for water quality are not available\r\nuntil the following March, these data must be reviewed during the following\r\nOctober review period, if not before.\r\nImplications\r\nBecause all of the data IMD collects is intended for use in supporting\r\nscience-based decisions as per our program’s five goals, and is intended for use\r\nin planning (the decisions of which are subject to public comment as per NEPA\r\nrequirements), this means that by default:\r\nAll analytical work we do should be reproducible to the extent possible.\r\nAnalytical work includes both statistical analysis and reporting of data as\r\nwell as quality control procedures where data are tested against quality\r\nstandards and qualified or corrected as appropriate.\r\nFull reproducibility may not be possible in all cases, particularly where\r\nanalytical methods involve subject matter expertise to make informed\r\njudgments on how to proceed with analyses. In such cases, decisions should\r\nbe documented to ensure transparency.\r\nAll IMD data should be published with supporting documentation to allow for\r\nreproduction of results.\r\nAll IMD data should be evaluated to determine whether they are suitable for\r\ntheir intended use.\r\nAll IMD data should be published with information fully describing how data\r\nwere collected, processed, and evaluated.\r\nAll data should be published in open formats that support the FAIR\r\nprinciples (Findable, Accessible, Interoperable, and Resuable).\r\nScope\r\n(for the NPS Inventory & Monitoring Program)\r\nGeneral Studies\r\nAny project that involves the collection of scientific\r\ndata for use in supporting decisions to be made by NPS personnel. General study\r\ndata may or may not be collected based on documented or peer-reviewed study\r\nplans or defined quality standards, but are in most cases purpose-driven and\r\nresultant information should be evaluated for the suitability for—and prior\r\nto—their use in decision support. These data may be reused for secondary\r\npurposes including similar decisions at other locations or times and/or portions\r\nof general study data may be reused or contribute to other scientific work\r\n(observations from a deer browsing study may be contribute to an inventory or\r\nmay be used as ancillary data to explain monitoring observations).\r\n\r\n\r\n\r\nFigure 1: Workflow for data collection, processing, dissemination, and use for general studies. Teal-colored boxes are subject to reproducibility requirements.\r\n\r\n\r\n\r\nVital Signs Monitoring\r\nVital signs monitoring data are collected by\r\nIMD and park staff to address specific monitoring objectives following methods\r\ndesigned to ensure long-term comparability of data. Procedures are established\r\nto ensure that data quality standards are maintained in perpetuity. However,\r\nbecause monitoring data are collected over long periods of time in dynamic\r\nsystems, the methods employed may differ from those prescribed in monitoring\r\nprotocols, procedures, or sampling plans, and any deviations (and resultant\r\nmitigations to the data) must be documented. Data should be evaluated to ensure\r\nthat they meet prescribed standards and are suitable for analyses designed to\r\ntest whether monitoring objectives have been met. Monitoring data may be reused\r\nfor secondary purposes including synthesis reports and condition assessments,\r\nand portions of monitoring data may contribute to inventories.\r\n\r\n\r\n\r\nFigure 2: Workflow for data collection, processing, dissemination, and use for vital sign monitoring efforts. Teal-colored boxes are subject to reproducibility requirements.\r\n\r\n\r\n\r\nInventory Studies\r\nInventory study data are similar to general study data\r\nin that they are time- and area-specific efforts designed to answer specific\r\nmanagement needs as well as broader inventory objectives outlined in\r\nproject-specific study plans and inventory science plans. Inventory studies\r\ntypically follow well-documented data collection methods or procedures, and\r\nresultant data should be evaluated for whether they are suitable for use in\r\nsupporting study-specific and broader inventory-level objectives. Inventory\r\nstudy data are expected to be reused to meet broader inventory level goals, but\r\nmay also support other scientific work and decision support.\r\n\r\n\r\n\r\nFigure 3: Workflow for data collection, processing, dissemination, and use for inventory studies. Teal-colored boxes are subject to reproducibility requirements.\r\n\r\n\r\n\r\nAmerican Statistical Association (ASA). 2017. Recommendations to funding agencies for supporting reproducible research. https://www.amstat.org/asa/files/pdfs/POL-ReproducibleResearchRecommendations.pdf.\r\nPlesser, H. E. 2017. Reproducibility vs. Replicability: A brief history of a confused terminology. Front. Neuroinform. 11:76. https://doi.org/10.3389/fninf.2017.00076.\r\n\r\n\r\n\r\n",
      "last_modified": "2024-04-29T11:07:51-06:00"
    },
    {
      "path": "PurposeAndScope.html",
      "title": "National Park Service Data Release Reports",
      "description": "Resources and Guides for generating NPS DRRs associated with data packages\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nBackground\r\nDefinitions\r\nPolicy Requirements\r\nNPS Requirements\r\nNPS Guidelines\r\nI&M Requirements\r\n\r\nImplications\r\nScope\r\nGeneral Studies\r\nVital Signs Monitoring\r\nInventory Studies\r\n\r\n\r\nBackground\r\nRecognizing the broad move toward open science, we’ve seen the following changes\r\nin expectations (outside NPS) since the inception of the I&M program.\r\nBroad adoption of open-data and open-by-default practices.\r\nA move in the scientific disciplines toward considering and publishing data\r\nsets as independently-citable scientific works.\r\nRoutine assignment of digital object identifiers (DOIs) to datasets to facilitate location, reuse, and\r\ncitation of specific data\r\nIncreased transparency and reproducibility in the processing and analysis of\r\ndata.\r\nEstablishment of peer reviewed “data journals” dedicated to publishing data sets\r\nand associated documentation designed to facilitate their reuse.\r\nExpectation that science-based decisions are based on peer-reviewed,\r\nreproducible, and open science by default.\r\nData release reports are designed to parallel external peer-reviewed scientific\r\njournals dedicated to facilitate reuse of reproducible scientific data, in\r\nrecognition that the primary reason IMD data are collected is to support\r\nscience-based decisions.\r\nNote that publication in a data release report series (not mandated) is distinct\r\nfrom requirements to document data collection, processing, and quality\r\nevaluation (mandated; see below). The establishment of a data release report\r\nseries is intended to facilitate and encourage this type of reporting in a\r\nstandard format, and in a manner commensurate with current scientific norms.\r\nA template for creating data release reports to be published in the DRR series. We have also\r\ndeveloped procedures for authoring DRR reports in Microsoft Word and porting\r\nthem to the appropriate format.\r\nDefinitions\r\nReproducibility. The degree to which scientific information, modeling, and\r\nmethods of analysis could be evaluated by an independent third party to arrive\r\nat the same, or substantially similar, conclusion as the original study or\r\ninformation, and that the scientific assessment can be repeated to obtain\r\nsimilar results (Plesser 2017). A study is reproducible if you can take the original data and the\r\ncomputer code used to analyze the data and reproduce all of the numerical findings\r\nfrom the study. This may initially sound like a trivial task but experience has shown that\r\nit’s not always easy to achieve this seemingly minimal standard (ASA 2017, Plesser 2017).\r\nTransparency. Full disclosure of the methods used to obtain, process,\r\nanalyze, and review scientific data and other information products, the\r\navailability of the data that went into and came out of the analysis, and the\r\ncomputer code used to conduct the analysis. Documenting this information is\r\ncrucial to ensure reproducibility and requires, at minimum, the sharing of\r\nanalytical data sets, relevant metadata, analytical code, and related software.\r\nFitness for Use. The utility of scientific information (in this case a\r\ndataset) for its intended users and its intended purposes. Agencies must review\r\nand communicate the fitness of a dataset for its intended purpose, and should\r\nprovide the public sufficient documentation about each dataset to allow data\r\nusers to determine the fitness of the data for the purpose for which third\r\nparties may consider using it.\r\nDecisions. The type of decisions that must be based on publicly-available,\r\nreproducible, and peer-reviewed science has not been defined. At a minimum it\r\nincludes any influential decisions, but it may also include any decisions\r\nsubject to public review and comment.\r\nDescriptive Reporting. The policies listed above are consistent in the\r\nrequirement to provide documentation that describes the methods used to collect,\r\nprocess, and evaluate science products, including data. Note that this is\r\ndistinct from (and in practice may significantly differ from) prescriptive\r\ndocuments such as protocols, procedures, and study plans. Descriptive reporting\r\nshould cite or describe relevant science planning documents, methods used,\r\ndeviations, and mitigations. In total, descriptive reporting provides a clear\r\n“line of sight” on precisely how data were collected, processed, and evaluated.\r\nAlthough deviations may warrant revisions to prescriptive documents, changes in\r\nprescriptive documents after the fact do not meet reproducibility and\r\ntransparency requirements.\r\nPolicy Requirements\r\nNPS Requirements\r\nDO11B-a, DO 11B-b, OMB M-05-03 (Peer review and information quality):\r\nScientific information must be appropriately reviewed prior to use in\r\ndecision-making, regulatory processes, or dissemination to the public,\r\nregardless of media.\r\nAs per OMB M-05-03 “scientific information” includes factual inputs, data,\r\nmodels, analyses, technical information, or scientific assessments related\r\nto such disciplines as the behavioral and social sciences, public health and\r\nmedical sciences, life and earth sciences, engineering, or physical\r\nsciences.\r\nMethods for producing information will be made transparent, to the maximum\r\nextent practicable, through accurate documentation, use of appropriate\r\nreview, and verification of information quality.\r\nOMB M-19-15 (Updates to Implementing the Information Quality Act):\r\nFederal agencies must collect, use, and disseminate information that is fit\r\nfor its intended purpose.\r\nAgencies must conduct pre-dissemination review of quality [of scientific\r\ninformation] based on the likely use of that information. Quality\r\nencompasses utility, integrity, and objectivity, defined as follows: a)\r\nUtility – utility for its intended users and its intended purposes, b)\r\nIntegrity – refers to security, and c) Objectivity – accurate, reliable, and\r\nunbiased as a matter of presentation and substance.\r\nAgencies should provide the public with sufficient documentation about each\r\ndataset released to allow data users to determine the fitness of the data\r\nfor the purpose for which third parties may consider using it. Potential\r\nusers must be provided with sufficient information to understand… the\r\ndata’s strengths, weaknesses, analytical limitations, security requirements,\r\nand processing options.\r\nReproducibility requirements for Influential Information. Note that because\r\nthis may not be determined at the time of collection, processing, or\r\ndissemination this should be the default for NPS scientific activities:\r\nAnalyses must be disseminated with sufficient descriptions of data and\r\nmethods to allow them to be reproduced by qualified third parties who\r\nmay want to test the sensitivity of agency analyses. This is a higher\r\nstandard than simply documenting the characteristics of the underlying\r\ndata, which is required for all information.\r\nComputer code used to process data should be made available to the\r\npublic for further analysis. In the context of results generated, for\r\nexample, a statistical model or machine augmented learning and decision\r\nsupport, reproducibility requires, at a minimum transparency about the\r\nspecific methods, design parameters, equations or algorithms,\r\nparameters, and assumptions used.\r\n\r\nReports, data, and computer code used, developed, or cited in the analysis\r\nand reporting of findings must be made publicly available except where\r\nprohibited by law.\r\nNPS Guidelines\r\nNPS guidelines on Use of Scientific Information\r\nMultiple policy and guidance documents require the use of best available science in decision-making. Additional requirements include:\r\nSO 3369 (Promoting Open Science):\r\nDefines “best available science” as publicly-available, reproducible, and peer reviewed. Requires that any decisions or scientific conclusions must prioritize the use of publicly-available, reproducible, and peer-reviewed science. Decisions or conclusions not based on such must include an explanation of why the alternative is the best available information. Effective as of 28 September 2018 with no transition period.\r\nDO 11B (Ensuring Objectivity, Utility, and Integrity of Information Used and Disseminated by the National Park Service):\r\nThe NPS will ensure that information it releases to the public or utilizes in management decisions will be developed from reliable data sources that provide the highest quality of information at each stage of information development.\r\nI&M Requirements\r\nNPS-75 (Inventory and Monitoring Guidelines):\r\nAn annual summary report documenting the condition of park resources should\r\nbe developed as part of the annual revision of the parks Resource Management\r\nPlan.\r\nAn annual report provides a mechanism for reviewing and making\r\nrecommendations for revisions in the [Protocol/SOPs].\r\n[Inventory] data obtained should be archived in park records and, when\r\nappropriate, a report should be written summarizing findings.\r\nReporting requirements as per IMD directive\r\nIMD Reporting and Analysis Guidance\r\nAnnual Analyses Required of all Monitoring Protocols: Conduct an annual data review\r\nto address whether there is any unexpected variability or outliers, and\r\nwhether any protocol changes or additional studies may be needed. Part of\r\nthe review must assess the data against standards defined in the protocol\r\nnarrative, data quality standards document or quality assurance plan, and\r\ndocument whether those standards were met. When data are not available for\r\nreview during the year they are collected (for example, when data have been\r\nsubmitted to a lab for analysis), review must be conducted the year the data\r\nare available. For example, if water quality and quantity data are typically\r\nreviewed in October and lab results for water quality are not available\r\nuntil the following March, these data must be reviewed during the following\r\nOctober review period, if not before.\r\nImplications\r\nBecause all of the data IMD collects is intended for use in supporting\r\nscience-based decisions as per our program’s five goals, and is intended for use\r\nin planning (the decisions of which are subject to public comment as per NEPA\r\nrequirements), this means that by default:\r\nAll analytical work we do should be reproducible to the extent possible.\r\nAnalytical work includes both statistical analysis and reporting of data as\r\nwell as quality control procedures where data are tested against quality\r\nstandards and qualified or corrected as appropriate.\r\nFull reproducibility may not be possible in all cases, particularly where\r\nanalytical methods involve subject matter expertise to make informed\r\njudgments on how to proceed with analyses. In such cases, decisions should\r\nbe documented to ensure transparency.\r\nAll IMD data should be published with supporting documentation to allow for\r\nreproduction of results.\r\nAll IMD data should be evaluated to determine whether they are suitable for\r\ntheir intended use.\r\nAll IMD data should be published with information fully describing how data\r\nwere collected, processed, and evaluated.\r\nAll data should be published in open formats that support the FAIR\r\nprinciples (Findable, Accessible, Interoperable, and Resuable).\r\nScope\r\n(for the NPS Inventory & Monitoring Program)\r\nGeneral Studies\r\nAny project that involves the collection of scientific\r\ndata for use in supporting decisions to be made by NPS personnel. General study\r\ndata may or may not be collected based on documented or peer-reviewed study\r\nplans or defined quality standards, but are in most cases purpose-driven and\r\nresultant information should be evaluated for the suitability for—and prior\r\nto—their use in decision support. These data may be reused for secondary\r\npurposes including similar decisions at other locations or times and/or portions\r\nof general study data may be reused or contribute to other scientific work\r\n(observations from a deer browsing study may be contribute to an inventory or\r\nmay be used as ancillary data to explain monitoring observations).\r\n\r\n\r\n\r\nFigure 1: Workflow for data collection, processing, dissemination, and use for general studies. Teal-colored boxes are subject to reproducibility requirements.\r\n\r\n\r\n\r\nVital Signs Monitoring\r\nVital signs monitoring data are collected by\r\nIMD and park staff to address specific monitoring objectives following methods\r\ndesigned to ensure long-term comparability of data. Procedures are established\r\nto ensure that data quality standards are maintained in perpetuity. However,\r\nbecause monitoring data are collected over long periods of time in dynamic\r\nsystems, the methods employed may differ from those prescribed in monitoring\r\nprotocols, procedures, or sampling plans, and any deviations (and resultant\r\nmitigations to the data) must be documented. Data should be evaluated to ensure\r\nthat they meet prescribed standards and are suitable for analyses designed to\r\ntest whether monitoring objectives have been met. Monitoring data may be reused\r\nfor secondary purposes including synthesis reports and condition assessments,\r\nand portions of monitoring data may contribute to inventories.\r\n\r\n\r\n\r\nFigure 2: Workflow for data collection, processing, dissemination, and use for vital sign monitoring efforts. Teal-colored boxes are subject to reproducibility requirements.\r\n\r\n\r\n\r\nInventory Studies\r\nInventory study data are similar to general study data\r\nin that they are time- and area-specific efforts designed to answer specific\r\nmanagement needs as well as broader inventory objectives outlined in\r\nproject-specific study plans and inventory science plans. Inventory studies\r\ntypically follow well-documented data collection methods or procedures, and\r\nresultant data should be evaluated for whether they are suitable for use in\r\nsupporting study-specific and broader inventory-level objectives. Inventory\r\nstudy data are expected to be reused to meet broader inventory level goals, but\r\nmay also support other scientific work and decision support.\r\n\r\n\r\n\r\nFigure 3: Workflow for data collection, processing, dissemination, and use for inventory studies. Teal-colored boxes are subject to reproducibility requirements.\r\n\r\n\r\n\r\nAmerican Statistical Association (ASA). 2017. Recommendations to funding agencies for supporting reproducible research. https://www.amstat.org/asa/files/pdfs/POL-ReproducibleResearchRecommendations.pdf.\r\nPlesser, H. E. 2017. Reproducibility vs. Replicability: A brief history of a confused terminology. Front. Neuroinform. 11:76. https://doi.org/10.3389/fninf.2017.00076.\r\n\r\n\r\n\r\n",
      "last_modified": "2024-04-29T11:07:51-06:00"
    }
  ],
  "collections": []
}
